\documentclass[a4paper]{scrartcl}

\usepackage[style = authoryear, backend = biber]{biblatex} 
\addbibresource{../ML_course.bib}


\usepackage{microtype}
\usepackage{amsmath, amssymb, amsthm, mathtools}
\usepackage{enumitem}
\usepackage{booktabs}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\DeclarePairedDelimiter\norm\lVert\rVert
\DeclarePairedDelimiter\abs\lvert\rvert

\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\reals}{\mathcal{R}}

\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\diag}{diag}

\title{Machine learning in empirical economic research}
\subtitle{Problem set}
\author{Andreas Dzemski}
\date{\today}

\begin{document}
\maketitle

\newcounter{CounterProblem}

\section*{\stepcounter{CounterProblem}Problem~\arabic{CounterProblem}}

The Ridge regressor is given by 
\begin{align*}
	\hat{\beta}^{\text{ridge}} = \argmin_{\beta_0 \in \mathbb{R}, \beta_1 \in \mathbb{R}^{p_n}} \sum_{i=1}^n \left(y - \beta_0 - x_i'\beta_1\right)^2 + \lambda \lVert \beta_1 \rVert_2^2 
\end{align*}
\begin{enumerate}
	\item Show that Ridge regression minimizes a strictly convex function and conclude that $\hat{\beta}^{\text{ridge}}$ is always uniquely defined.
	\item Show that 
	\begin{align*}
		\hat{\beta}^{\text{ridge}} = \left(\mathbf{X}'\mathbf{X} + \lambda \diag\big((0, 1, \dotsc 1)'\big) \right)^{-1} \mathbf{X}' \mathbf{y}
	\end{align*}
	\item Suppose that $p_n < n$ and that the off-diagonal elements of $\mathbf{X}'\mathbf{X}/n$ are zero. In that case $\hat{\beta}^{\text{ols}}$ is defined. Show that 
	\begin{align*}
		\left( \frac{(\mathbf{X}'\mathbf{X}/n)_{jj}}{(\mathbf{X}'\mathbf{X}/n)_{jj} + \lambda} \right) \hat{\beta}^{\text{ols}}.
	\end{align*}
	Relate this result to the ``shrinkage'' property of Ridge regression.
\end{enumerate}

\section*{\stepcounter{CounterProblem}Problem~\arabic{CounterProblem}}

The $L_q$ penalized least squares estimator is given by 
\begin{align*}
	\hat{\beta}^{L_q} = \argmin_{\beta_0 \in \mathbb{R}, \beta_1 \in \mathbb{R}^{p_n}} \sum_{i=1}^n \left(y - \beta_0 - x_i'\beta_1\right)^2 + \lambda \lVert \beta_1 \rVert_q^q. 
\end{align*}
\begin{enumerate}
	\item Show that this definition is equivalent to 
	\begin{align*}
		\hat{\beta}^{L_q} =& \argmin_{\beta_0 \in \mathbb{R}, \beta_1 \in \mathbb{R}^{p_n}} \sum_{i=1}^n \left(y - \beta_0 - x_i'\beta_1\right)^2
	\\
		\text{subject to:} & \quad  \lVert \beta_1 \rVert_q \leq s_{\lambda}
	\end{align*}
	for some $s_{\lambda}$.
	\item Let $\mathbf{X}_{+1}$ denote the design matrix including intercept ($n \times (p_n + 1)$ matrix). Suppose that $\mathbf{X}_{+1}'\mathbf{X}_{+1}/n$ has full rank. Let $\hat{\beta}^{\text{ols}}$ denote the OLS estimator 
	\begin{align*}
		\hat{\beta}^{\text{ols}} = \left(\mathbf{X}_{+1}'\mathbf{X}_{+1} \right)^{-1} \mathbf{X}_{+1}' \mathbf{y}.
	\end{align*}
	Show that the contour sets 
	\begin{align*}
		\big\{
	  		\beta \in \mathbb{R}^{p_n + 1} : \lVert \mathbf{y} - \mathbf{X}_{+1} \beta \rVert_2^2 = c 
		\big\}
	\end{align*}
	are empty or ellipsoids centered at $\hat{\beta}^{\text{ols}}$.
\end{enumerate}

\section*{\stepcounter{CounterProblem}Problem~\arabic{CounterProblem}}

We consider the the simulation excercise from the slides for the ``sparse uncorrelated'' design. The repository with the simulation code can be found at \url{https://github.com/adzemski/ML_notes}.

\begin{enumerate}
	\item In the Gaussian case $\hat{\beta}_1^{\text{ols}}$ is the MLE estimator. Explain the efficiency result for MLE estimators (Cram\'er-Rao lower bound). Theoretically, can $\hat{\beta}_1^{\text{post}}$ beat $\hat{\beta}_1^{\text{ols}}$ in terms of efficiency?
	\item 
	Extend the simulation excercise to simulate also the variance and the MSE of $\hat{\beta}_1^{\text{post}}$ and $\hat{\beta}_1^{\text{ols}}$. Which estimator is more efficient?
\end{enumerate}

\section*{\stepcounter{CounterProblem}Problem~\arabic{CounterProblem}}

Implement and simulate the double selection procedure from \textcite{belloni2014inference} for the ``sparse correlated'' desgin from the slides. The repository with the simulation code can be found at \url{https://github.com/adzemski/ML_notes}.

\end{document}