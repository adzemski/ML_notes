\documentclass[xcolor=dvipsnames, handout]{beamer}
% \documentclass[xcolor=dvipsnames]{beamer}

\usepackage[]{graphicx}
\usepackage[]{color}

\usetheme{Singapore}
\setbeamertemplate{itemize subitem}[triangle]

% for handout version (uncomment to get handout)
\usepackage{pgfpages}
% \pgfpagesuselayout{2 on 1}[a4paper,border shrink=2mm]

% \usepackage{tabularx}
% \usepackage{caption}
\usepackage{booktabs}
\usepackage{colortbl, xcolor}
% \usepackage{comment}
% \setbeamertemplate{theorems}[numbered]

% bibliography
\usepackage[backend = biber, style = authoryear]{biblatex}
\addbibresource{../ML_course.bib}

\usepackage{amssymb, amsmath, amsthm, bm, mathtools}
\usepackage{graphicx}

% for some graphs
\usepackage{tabularx}
% \usepackage{tikz, xcolor}
% \usetikzlibrary{patterns, shapes, backgrounds, positioning, arrows}
%\usepackage{gnuplot-lua-tikz}

\newcommand{\tabitem}{~~\llap{\textbullet}~~}

\newtheorem{proposition}{Proposition}

\author[Dzemski]{Andreas Dzemski\inst{1}}
\institute{\inst{1} University of Gothenburg}
\title{Mini-course Machine Learning in Empirical Economic Research}
\subtitle{Lecture 2: Introduction to supervised learning}
\date{June 5, 2019}
%
\renewcommand*{\bibfont}{\scriptsize}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\makebox[1em][l]{\ding{51}}}%
\newcommand{\xmark}{\makebox[1em][l]{\ding{55}}}%

\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator{\bias}{bias}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\diag}{Diag}
\DeclareMathOperator{\cor}{cor}
\DeclareMathOperator{\argmin}{arg\,min}
%
%
%
\begin{document}
\maketitle

\begin{frame}{Supervised learning}
\begin{itemize}
    \item outcome $y$, regressors/features $x$
    \pause 
    \item $f(x)$ prediction of $y$ using prediction rule $f$
    \pause 
    \item loss function $L\left(y, f(x))\right)$
    \item quadratic loss
    \begin{align*}
      L\left(y, f(x)\right) =  \left(y - f(x) \right)^2 
    \end{align*}
    \pause 
    \item function class $\mathcal{F}$ of possible $f$
    \pause
    \item subclasses $\mathcal{F}_{\lambda} \subset \mathcal{F}$
    \item $\lambda$ = tuning parameter
  \end{itemize}  
\end{frame}

\begin{frame}{Machine learning as data-driven model selection}
\begin{columns}[t, onlytextwidth]
    \column{.47\textwidth}
        \begin{block}{``Traditional'' econometrics}
        \begin{itemize}
          \item<1-> 
          specify $\mathcal{F}$
          \item<2-> 
          $\mathcal{F}$ is typically very ``small''
          \item<3-> 
          fit ``best'' $f \in \mathcal{F}$
        \end{itemize}
        \end{block}
    \column{.47\textwidth}
        \begin{block}{Machine Learning}
        \begin{itemize}
          \item<1-> 
          specify $\mathcal{F}$ and $\lambda \mapsto \mathcal{F}_{\lambda}$
          \item<2-> 
          $\mathcal{F}$ is typically ``large''
          \item<4-> 
          use data to determine a ``good'' $\hat{\lambda}$
          \item<4->
          typically $\mathcal{F}_{\hat{\lambda}}$ is ``small'' or at least well-behaved
          \item<4-> 
          fit ``best'' $f \in \mathcal{F}_{\hat{\lambda}}$
        \end{itemize}
      \end{block}
\end{columns}
\end{frame}

\begin{frame}{Fitting $f \in \mathcal{F}_{\lambda}$}
\begin{itemize}
  \item training data $\mathcal{T} = \{ (y_i, x_i') \}_{i=1}^n$
  \item \emph{training error} = average loss on $\mathcal{T}$
  \begin{align*}
     \overline{\text{err}} (f) = \frac{1}{n} \sum_{i=1}^n L\left(y_i, f(x_i) \right) 
   \end{align*} 
   \pause
   \item fit $f$ with minimal training error
   \begin{align*}
     \hat{f} = \hat{f}^{\lambda} = \argmin_{f \in \mathcal{F}^{\lambda}} \overline{\text{err}} (f) 
   \end{align*}
   \pause 
   \item conceptually this is the same as what we do in econometrics! 
   \begin{itemize}
     \pause\item what loss functions do we use in econometrics?
     \pause\item how do we call the fitted training error?
     \pause\item estimation as constrained optimization?
   \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Intuition for $\lambda$}
\begin{itemize}
  \item assume $\lambda \in \mathbb{R}$
  \item often $\lambda$ performs \emph{regularization}
  \pause
  \item let $\mathcal{C}(f)$ measure ``complexity'' of $f$
  \begin{align*}
    \mathcal{F}_{\lambda}(f) = \left\{f \in \mathcal{F} : \mathcal{C}(f) \leq \lambda \right\}   
  \end{align*} 
  \pause 
  \item example: fitting a smooth curve over an interval
  \begin{align*}
    \mathcal{C}(f) = \int_{x \in [a,b]} \lvert f''(x) \rvert^2 \, dx
  \end{align*}
  \pause 
  \item implies nesting of classes: for $\lambda_1 \leq \lambda_2 \leq \dotsb$
  \begin{align*}
    \mathcal{F}_{\lambda_1}\subset \mathcal{F}_{\lambda_2} \subset \dotsb
  \end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Choice of $\lambda$ = trade-off}
\begin{columns}[t, onlytextwidth]
    \column{.47\textwidth}
        \begin{block}{small $\lambda$}
        \begin{itemize}
          \item<1-> 
          small $\mathcal{F}_{\lambda}$
          \item<2-> 
          consisting of ``simple'' models
          \item<3-> 
          potentially poor approximation (high bias)
          \item<4-> 
          easy to estimate (low variance)
        \end{itemize}
        \end{block}
    \column{.47\textwidth}
        \begin{block}{large $\lambda$}
        \begin{itemize}
          \item<1-> 
          large $\mathcal{F}_{\lambda}$
          \item<2-> 
          consisting of ``complex'' models
          \item<3-> 
          potentially better approximation (low bias)
          \item<4-> 
          hard to estimate (high variance)
        \end{itemize}
      \end{block}
\end{columns}
\begin{itemize}
  \item<5-> we order models according to the ``Occam's razor'' principle 
\end{itemize}
\end{frame}

\begin{frame}{Determining $\hat{\lambda}$}
\begin{itemize}
  \item our objective = prediction
  \item good $\lambda$ leads to estimator with good predictive properties
  \pause 
  \item $\E_{\mathcal{T}}$ = expectation operator wrt training sample
  \item $E_{y,x}$ = integral wrt probability measure of a new $(y, x')$ observation  
  \pause 
  \item expected prediction error
  \begin{align*}
    EPE(\hat{f}^{\lambda}) = \E_{\mathcal{T}} E_{y,x} L\left(y, \hat{f}^{\lambda}(x) \right)
  \end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Training error does not estimate prediction error}
\begin{figure}
  \includegraphics[width = 0.8\textwidth]{../screen_shots/ESL_fig_2_11.pdf}
   \caption{Figure~2.11 from \textcite{hastie2009elements}}
\end{figure}
\end{frame}


\begin{frame}{Overfitting}
\begin{itemize}
  \item $\overline{\text{err}} (f)$ is an \emph{optimistic} estimate of prediction error
  \begin{itemize}
    \item the training error does not account for cost of uncertainty (variance)
  \end{itemize}
\pause
  \item minimizing training error $\overline{\text{err}} (f)$ over all of $\mathcal{F}$ leads to \emph{overfitting}
\pause 
  \item introductory econometrics courses often don't discuss overfitting, why? 
\pause
  \item Two lessons: 
  \begin{itemize}
    \pause\item estimation: fit $\overline{\text{err}} (f)$ keeping $\lambda$ fixed
    \pause\item model testing: in-sample fit (aka $\overline{\text{err}}$ or $R^2$) is not a good measurement of fit
    \begin{itemize}
      \item even if you are interested in predictive power don't look at $R^2$!
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{How do we validate fit in empirical economic research?}
\begin{quote}
``For many years, economists have reported in-sample goodness-of-fit measures using the excuse that we had small datasets. But now that larger datasets have become available, there is no reason not to use separate training and testing sets.'' \parencite{varian2014big}
\end{quote}
\end{frame}


\begin{frame}{Sample-splits in data-rich environments}
Use independent samples for three distinct tasks: 
\begin{description}
\item[Training] fit $f \in \mathcal{F}_{\lambda}$
\begin{itemize}
  \item this is how we fit $\hat{f}^{\lambda}$ for candidate $\lambda$
  \item minimize training error
\end{itemize}
\item[Validation] estimate EPE for given $\hat{f}^{\lambda}$
\begin{itemize}
  \item this is how we evaluate suitability of $\lambda$ candidates
  \item best candidate = $\hat{\lambda}$
\end{itemize}
\item[Test] estimate predictive power of final model $\hat{f}^{\hat{\lambda}}$
\end{description}
\end{frame}

\begin{frame}{Estimating EPE from independent validation data set}
Validation data set 
\begin{align*}
  \{(y^*_i, (x_i^*)')\}_{i=1}^{n^*}
\end{align*}
\pause 
Estimated $EPE$
\begin{align*}
  \widehat{EPE}\left(f\right) = \frac{1}{n^*} \sum_{i=1}^{n^*} L\left(y_i, f(x^*_i)\right)   
\end{align*}
\pause 
\begin{itemize}
  \item $\widehat{EPE}\big(\hat{f}^{\lambda}\big)$ estimates the \emph{conditional} expected prediction error
  \begin{align*}
    EPE_{\mathcal{T}} \big(\hat{f}^{\lambda}\big) = E_{y, x} L \big(y, \hat{f}^{\lambda}(x) \big)
  \end{align*}
\end{itemize}
\end{frame}


\begin{frame}{Estimating EPE using cross-validation}
\begin{itemize}
  \item validation step without need for validation data set
  \pause
  \item $k$-fold cross-validation (CV) based on sample split \emph{of the training sample}
  \pause 
  \item example with $k=5$ folds
  \begin{figure}
  \includegraphics[width = 0.8\textwidth]{../screen_shots/ESL_fig_p242.pdf}
   \caption{Figure from p242 of \textcite{hastie2009elements}}
\end{figure}
\end{itemize}
\end{frame}

\begin{frame}{$k$-fold cross-validation}
Random partition $\{\kappa_j\}_{j = 1}^k$
\begin{align*}
\bigcup_{j = 1}^k \kappa_j = \{1, \dotsc, n\}
\end{align*}
$\hat{f}^{\lambda}_{-\kappa_j}$ = fit $f \in \mathcal{F}_{\lambda}$ on subsample 
\begin{align*}
  \{1, \dotsc, n\} \setminus \kappa_j
\end{align*}
evaluate $\hat{f}_{-\kappa_j}$ on $\kappa_j$
\begin{align*}
  CV_j(\lambda) = \frac{1}{\lvert\kappa_j\rvert} \sum_{i \in \kappa_j} L\big(\hat{f}^{\lambda}_{-\kappa_j}(x_i), y_i \big)
\end{align*}
\end{frame}

\begin{frame}{$k$-fold cross-validation}
Average over partitions
\begin{align*}
  CV(\lambda) = \frac{1}{k} \sum_{j=1}^k CV_j(\lambda)
\end{align*}
\pause
\begin{itemize}
  \item $CV(\lambda)$ is an estimate of the \emph{unconditional} expected prediction error
    \begin{align*}
    EPE \big(\hat{f}^{\lambda}\big) = \E_{\mathcal{T}} E_{y, x} L \big(\hat{f}^{\lambda}(x), y \big)
  \end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Standard error of cross-validation}
\begin{align*}
  SE_{CV}(\lambda) =& \text{sample std of $\{CV_j(\lambda)\}_{j=1}^k$}/{\sqrt{k}}
\\
  =& \sqrt{\frac{1}{k} \sum_{j=1}^k\left( CV_j(\lambda) - CV(\lambda)\right)^2}/\sqrt{k}
\end{align*}
\begin{itemize}
  \item ad-hoc measure of the ``sampling error'' of cross-validation
\end{itemize}
\end{frame}

\begin{frame}{Choice of number of folds}
\begin{itemize}
  \item typically values $k = 5, 10, n$
  \item $k = n$: leave-one-out cross-validation
\end{itemize}
\begin{columns}[t, onlytextwidth]
    \column{.47\textwidth}
        \begin{block}{few folds}
        \begin{itemize}
          \item<1-> 
          training step in CV uses \emph{much less} observations than $n$
          \item<2-> 
          biased
          \item<3-> 
          low variance (training sets not very similar)
        \end{itemize}
        \end{block}
    \column{.47\textwidth}
        \begin{block}{leave-one-out}
        \begin{itemize}
          \item<1-> 
          training step in CV almost $n$ observations
          \item<2-> 
          approximately unbiased
          \item<3-> 
          high variance (training sets very similar)        
        \end{itemize}
      \end{block}
\end{columns}
\end{frame}

\begin{frame}{Example}
  \begin{figure}
  \includegraphics{../../R_code/penalized_regression/output_for_slides/cv_example_lambda_path.pdf}
   \caption{Cross-validation $CV(\lambda)$, $k = 10$ folds. Here: small $\lambda$ = simple model, large $\lambda$ = complex model.}
\end{figure}
\end{frame}

\begin{frame}{Choice of $\lambda$ based on CV}
\begin{itemize}
  \item optimize $\lambda$ for prediction: minimial $\lambda$
\begin{align*}
  \hat{\lambda} = \hat{\lambda}_{\min} = \argmin_{\lambda} CV(\lambda)
\end{align*}
\pause 
\item suppose large $\lambda$ = simpler model (more regularization)
\item impose additional regularization: 1-$\sigma$ rule
\begin{align*}
  \hat{\lambda} = & \hat{\lambda}_{1\sigma} 
\\
  =&  \max \{\lambda \in \mathbb{R} : CV(\lambda) \leq CV(\hat{\lambda}_{\min}) +  SE_{CV}(\hat{\lambda}_{\min}) \}
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Example}
  \begin{figure}
  \includegraphics{../../R_code/penalized_regression/output_for_slides/cv_example_lambda_choice.pdf}
   \caption{Optimal choice of $\lambda$, $k = 10$ folds. Here: small $\lambda$ = simple model, large $\lambda$ = complex model.}
\end{figure}
\end{frame}

\begin{frame}{Applying a supervised learning method}
\begin{itemize}
  \item have to choose a \emph{tuning parameter} $\lambda$
  \begin{itemize}
    \item regularization parameter
    \item hyperparameter
  \end{itemize}
  \item can be a vector
\end{itemize}
\pause 
\begin{description}[<+->]
  \item[Training the model] 
  \begin{itemize}
    \item 
    For given tuning parameter $\lambda$, fit the best model $\hat{f}^{\lambda}$ using training data.
    \item 
    $\hat{f}^{\lambda}$ is \emph{learned}
  \end{itemize}
  \item[Hyperparameter optimization] 
  \begin{itemize}
    \item choose $\lambda$
    \item find the $\lambda$ that gives $\hat{f}^{\lambda}$ with the best generalization (= out-of-sample) performance
    \begin{itemize}
      \item independent validation sample 
      \item cross-validation 
      \item other 
    \end{itemize}
    \item compute the training step many times
  \end{itemize}
\end{description}
\end{frame}

\begin{frame}
\begin{figure}
  \includegraphics[width = 0.5\textwidth]{../screen_shots/xkcd_ML.png}
   \caption{Source: https://xkcd.com/1838/}
\end{figure}
\end{frame}



\end{document}
